# Titanic Survival Prediction - Project Summary

## ğŸ¯ Project Overview

This project demonstrates a complete Machine Learning pipeline from data processing to model deployment, using the classic Titanic survival prediction dataset.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw Data      â”‚    â”‚  Processed Data â”‚    â”‚  Trained Model  â”‚
â”‚  (train.csv)    â”‚â”€â”€â”€â–¶â”‚ (processed/)    â”‚â”€â”€â”€â–¶â”‚ (models/)       â”‚
â”‚  (test.csv)     â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                       â”‚
                                â–¼                       â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   FastAPI       â”‚    â”‚   Docker        â”‚
                       â”‚   Application   â”‚    â”‚   Container     â”‚
                       â”‚   (src/api.py)  â”‚    â”‚   (Dockerfile)  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                       â”‚
                                â–¼                       â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   REST API      â”‚    â”‚   Production    â”‚
                       â”‚   Endpoints     â”‚    â”‚   Deployment    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
Titanic_Project/
â”œâ”€â”€ ğŸ“Š Data Processing
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ train.csv              # Original training data
â”‚   â”‚   â”œâ”€â”€ test.csv               # Original test data
â”‚   â”‚   â”œâ”€â”€ processed/             # Processed data (generated)
â”‚   â”‚   â””â”€â”€ submission.csv         # Predictions (generated)
â”‚   â””â”€â”€ src/process_data.py        # Data cleaning & feature engineering
â”‚
â”œâ”€â”€ ğŸ¤– Model Training
â”‚   â”œâ”€â”€ models/                    # Trained models (generated)
â”‚   â”‚   â”œâ”€â”€ titanic_random_forest.joblib
â”‚   â”‚   â””â”€â”€ model_info.txt
â”‚   â””â”€â”€ src/train.py              # Model training & evaluation
â”‚
â”œâ”€â”€ ğŸŒ API Deployment
â”‚   â”œâ”€â”€ src/api.py                # FastAPI application
â”‚   â”œâ”€â”€ Dockerfile                # Container configuration
â”‚   â”œâ”€â”€ docker-compose.yml        # Multi-container setup
â”‚   â””â”€â”€ .dockerignore             # Docker build optimization
â”‚
â”œâ”€â”€ ğŸ§ª Testing & Demo
â”‚   â”œâ”€â”€ test_api.py               # API test suite
â”‚   â”œâ”€â”€ demo_api.py               # Interactive demo
â”‚   â””â”€â”€ run_pipeline.py           # Complete pipeline runner
â”‚
â””â”€â”€ ğŸ“š Documentation
    â”œâ”€â”€ README.md                 # Project documentation
    â”œâ”€â”€ requirements.txt          # Python dependencies
    â””â”€â”€ PROJECT_SUMMARY.md        # This file
```

## ğŸ”„ Complete Pipeline

### 1. Data Processing (`src/process_data.py`)
- **Missing Value Handling**: Age (median), Fare (median), Embarked (mode), Cabin (Unknown)
- **Feature Engineering**:
  - Title extraction from Name
  - FamilySize = SibSp + Parch + 1
  - IsAlone binary indicator
  - Deck extraction from Cabin
- **Categorical Encoding**: One-hot encoding with `pd.get_dummies()`
- **Output**: Processed datasets ready for modeling

### 2. Model Training (`src/train.py`)
- **Algorithm**: RandomForestClassifier
- **Parameters**: n_estimators=100, max_depth=5, random_state=1
- **Evaluation**: 5-fold cross-validation
- **Metrics**: Accuracy, Classification Report, Confusion Matrix
- **Output**: Trained model saved with joblib

### 3. API Development (`src/api.py`)
- **Framework**: FastAPI
- **Endpoints**:
  - `GET /` - API information
  - `GET /health` - Health check
  - `GET /model_info` - Model details
  - `POST /predict` - Single prediction
  - `POST /predict_batch` - Batch predictions
  - `GET /docs` - Interactive documentation
- **Features**:
  - Automatic data preprocessing
  - Input validation with Pydantic
  - Error handling and logging
  - Probability scores

### 4. Containerization (`Dockerfile`)
- **Base Image**: Python 3.9-slim
- **Security**: Non-root user
- **Health Checks**: Automatic monitoring
- **Optimization**: Multi-stage build, .dockerignore

## ğŸš€ Deployment Options

### Local Development
```bash
# 1. Setup environment
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 2. Run pipeline
./run.sh

# 3. Start API
uvicorn src.api:app --reload --host 0.0.0.0 --port 8000
```

### Docker Deployment
```bash
# Option 1: Docker Compose (Recommended)
docker-compose up --build

# Option 2: Manual Docker
docker build -t titanic-api .
docker run -p 8000:8000 titanic-api
```

### Production Considerations
- **Load Balancing**: Use nginx or similar
- **Monitoring**: Prometheus + Grafana
- **Logging**: Structured logging with JSON
- **Security**: HTTPS, API keys, rate limiting
- **Scaling**: Kubernetes or Docker Swarm

## ğŸ“ˆ Model Performance

- **Cross-validation Score**: 83.16% (Â±3.85%)
- **Training Accuracy**: 83.95%
- **Features Used**: 27 engineered features
- **Model Size**: ~412KB (joblib format)

## ğŸ”§ Key Features

### Data Processing
- âœ… Handles missing values intelligently
- âœ… Creates meaningful engineered features
- âœ… Ensures feature consistency between train/test
- âœ… Scales to new data automatically

### Model Training
- âœ… Cross-validation for robust evaluation
- âœ… Comprehensive performance metrics
- âœ… Model persistence with metadata
- âœ… Reproducible results (random_state)

### API Design
- âœ… RESTful endpoints
- âœ… Input validation and error handling
- âœ… Automatic documentation (Swagger UI)
- âœ… Health checks and monitoring
- âœ… Batch processing capability

### Deployment
- âœ… Containerized for portability
- âœ… Environment isolation
- âœ… Easy scaling and replication
- âœ… Production-ready configuration

## ğŸ¯ Use Cases

1. **Educational**: Learn ML pipeline best practices
2. **Prototyping**: Quick model deployment
3. **Production**: Scalable ML service
4. **Integration**: Easy to integrate with other systems

## ğŸ”® Future Enhancements

- **Model Versioning**: MLflow integration
- **A/B Testing**: Multiple model comparison
- **Feature Store**: Centralized feature management
- **Monitoring**: Model drift detection
- **CI/CD**: Automated testing and deployment

## ğŸ“Š API Usage Examples

### Single Prediction
```bash
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "Pclass": 1,
    "Sex": "female",
    "SibSp": 1,
    "Parch": 0,
    "Age": 29.0,
    "Fare": 211.3375,
    "Embarked": "S",
    "Name": "Mrs. John Bradley Cumings"
  }'
```

### Batch Prediction
```bash
curl -X POST "http://localhost:8000/predict_batch" \
  -H "Content-Type: application/json" \
  -d '[{...passenger1...}, {...passenger2...}]'
```

This project demonstrates a production-ready ML pipeline that can be easily adapted for other classification problems.
